stats
---
Statistics
---
* Total http requests, bytes (currently 77G of gzipped WARCs)
* Complete crawl duration (start, end times)
* HTML pages fetched (ie HTTP 200), bytes
  bigquery: pages 4,056,693
* Links: total, internal
  bigquery: links 481,054,340, link classes 198,121,176
* mf2 classes, instances
* Pages with mf2, webmention endpoint, micropub endpoint, auth endpoint
  * Distinct endpoints
* Conversion time (WARC => BigQuery): ~60h, 5/25-5/27

from WARCs:
* total WARC file size (compressed, uncompressed)
* records
* requests
* responses
* URLs (request and response):
  * host
  * pay level domain
  * TLD
  * SSL or no
* HTTP status code
* HTTP version

from BigQuery pages:
* # pages
* response size
* min, max timestamp
* URLs (request and response):
  * host
  * pay level domain
  * TLD
  * https vs http
* response headers:
  * Content-Type, including encoding
  * Server
  * Content-Length
  * X-Powered-By
  * Last-Modified


canonical pages view
---
638974 in current view query

1927101
1927101 in new view
1911131 in NOT
3839650 in extras

1927101 + 1911131 = 3838232


duplicate pages
---
...because many warcs have dupe requests and responses for identical URL.
459 domains have them!
751878 URLs
1625130 rows

873252 to delete!

deleted 217043.

now 656209 with dupes.


example URLs in small site:
http://lobpreiscamp.ostwind.zone/anmeldung-erforderlich/
http://lobpreiscamp.ostwind.zone/events/lobpreis-camp-marienfried/
http://lobpreiscamp.ostwind.zone/events/lobpreistag/

attempts at delete query (none work yet)
STATE: insert into new table with ROW_NUMBER() 1?

# https://stackoverflow.com/a/3822833/186123
WITH partitions AS (
  SELECT ROW_NUMBER() OVER (PARTITION BY url ORDER BY fetch_time) row_num
  FROM indiemap.pages
  ORDER BY url
)
DELETE FROM partitions
WHERE row_num > 1;

# other
WITH dupes AS (
SELECT domain, url, fetch_time, COUNT(*) c
FROM indiemap.pages
GROUP BY domain, url, fetch_time
HAVING c > 1
ORDER BY c DESC
)
SELECT DISTINCT domain FROM dupes


# this finds 217043. ran it on 6/9 4:17pm
delete from indiemap.pages p
where p.url IN (
SELECT url
FROM indiemap.pages
GROUP BY url
HAVING COUNT(*) > 1
)
AND fetch_time > (SELECT MIN(fetch_time) from indiemap.pages where url = p.url)


select * from indiemap.pages p
where p.url in (
SELECT url
FROM indiemap.pages
GROUP BY url
HAVING COUNT(*) > 1
)
-- SELECT url, ROW_NUMBER() OVER (PARTITION BY url ORDER BY fetch_time) as row_num
-- FROM indiemap.pages
-- GROUP BY url, fetch_time
-- HAVING COUNT(*) > 1
-- )





stats
---
# run warc_stats.py
# write charsets section to charsets file
grep -E 'utf-?8' charsets |cut -d' ' -f -1

5756899 pages
5084713 canonical pages
( https://bigquery.cloud.google.com/results/indie-map:bquijob_3cce851a_15c9aa55bf9 )

